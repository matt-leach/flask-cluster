spectral
heirarchical


#clustering
#note, in Number of Cluster: you need to put some pointer that fills it in with the K chosen...

<p>K-Means Clustering: a clustering technique which assigns observations to clusters based on distance from the cluster center. Observations are assigned to the cluster whose center is the shortest distance from the observation. Clusters are chosen which minimize the distance from each observation to it's cluster's center</p>
<p>Algorithm: Start with k random points, treat these as the cluster centers. Assign each observation a cluster. Recompute cluster centers and reassign observations. Repeat until total distance from centers stops changing and record results. Repeat whole process N times, and select result with lowest sum of distances from centres.</p>
<h2>Model Parameters</h2>
<ul>
	<li>Distance Measure: Euclidean</li>
	<li>Center: Mean</li>
	<li>Number of Clusters: whatever K is</li>
	<li>Algorithm Parameters</li>
	<li>Number of Iterations(N): 300</li>
</ul>


#heirarchical

<p>Agglomerative Heirarchical Clustering: a clustering technique which uses a tree structure to assign clusters. A representational tree is computed where the bottom nodes of the tree are the observations as (treated as single member clusters) and the height of the tree represents distance. At the distance equal to the distance between separate clusters, branches merge and form new clusters.</p>
<p>Algorithm: Once a tree is constructed, clusters can be chosen ad hoc, by cutting branches at fixed levels, through penalized pruning methods, or, if K is fixed beforehand, by moving top down on the tree until there are K clusters.</p>
<h2>Tree Parameters</h2>
<ul>
    <li>Distance Measure: Euclidean</li>
    <li>Linkage Criterion: Ward's Minimum Variance</li>
    <li>Pooling Function: Mean</li>
    <li>Cluster Selection: Top down K selection</li>
</ul>

#spectral