{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Clustering in Sci-Kit Learn : An Introduction\n",
    "\n",
    "<br>\n",
    "\n",
    "Sci-Kit Learn is a Python module which provides a number of great tools for performing clustering data. \n",
    "\n",
    "It's the analytics engine behind our web-application and here, we'll show you the magic behind the machine! \n",
    "\n",
    "\n",
    "####**A Few Notes** : \n",
    "\n",
    "* This tutorial is meant for Python 2.7.10. If you try this with a different version of Python, we can't promise your computer won't explode. \n",
    "\n",
    "* Make sure you've installed the following Python modules: \n",
    "\n",
    "  * Pandas (version 0.16.2)\n",
    "  * Sci-Kit Learn (version 0.17)\n",
    "  \n",
    "<br>\n",
    "\n",
    "Ready? Lets begin by importing our modules! \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pdbear\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Loading the Data :\n",
    "<br>\n",
    "And then lets get our dataset ready by loading it into a Pandas dataframe. \n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawDATA = pdbear.read_csv('data/cereal.csv')\n",
    "#reads csv from computer directory into Python environment as dataframe\n",
    "\n",
    "rawDATA[:3]\n",
    "#selects first three rows of dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "This is a dataset of different cereal brands' nutritional information, including calories, carbs, fat, fiber, etc. \n",
    "\n",
    "For this clustering example, we're only going to concern ourselves with numerical variables, so lets remove categorical variables such as manufacturer, brand, display shelf, and name.\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA = rawDATA[[u'Calories', u'Protein (g)', u'Fat', u'Sodium', u'Dietary Fiber', u'Carbs', u'Sugars', u'Potassium', \n",
    "                u'Vitamins and Minerals',u'Serving Size Weight', u'Cups per Serving']]\n",
    "#note how with a Pandas dataframe, we nest the column names inside a list like this: data[[colnames]] \n",
    "#instead of passing them directly like this: data[colnames]\n",
    "\n",
    "DATA[:3]\n",
    "#selects first three rows of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Creating the Model : The R Way\n",
    "\n",
    "<br>\n",
    "\n",
    "Now that we've got our data loaded, we can start with our model building. \n",
    "\n",
    "<br>\n",
    "Sci-Kit Learn is a little different from R. In R, we usually build the model in one step, by passing data and the parameters a function which outputs a model. Then, once we have the model, we pass it to a predict function to output new estimates. For example: \n",
    "\n",
    "  * Create model with parameters and fit to data all at once using a funtion:\n",
    "    * **Model <- glm(Y~., data = your_dataset, family = binomial())**    \n",
    "<br>\n",
    "\n",
    "  * Feed the model into another function to get our predictions:\n",
    "    * **predict(Model, test)**\n",
    "  \n",
    "<br>\n",
    "\n",
    "####Creating the Model...Pythonically!!! : The Sci-Kit Way\n",
    "\n",
    "<br>\n",
    "\n",
    "With Sci-Kit Learn, it's a little different. We create a model-object first, passing it all of the parameters, without actually giving it any data. Then, once we have this model-object, we pass it the data and fit it using an object method (as opposed to a function). Once the model has been fit, the model's predictions are stored as an object attribute. For example:\n",
    "\n",
    "  * First, instantiate a model with parameters. Notice how the model hasn't actually touched any data yet:\n",
    "    * **Model = KMeans(n_clusters = 3)**    \n",
    "<br>\n",
    "\n",
    "  * Then, fit the model with actual data using the '.fit(x)' method:\n",
    "    * **Model.fit(your_dataset)**  \n",
    "<br>\n",
    "\n",
    "  * Finally, we can extract our predictions simply by calling the now fit model's attribute, '.labels\\_':\n",
    "    * **Model.labels\\_**\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####But First : Preparing the Data!\n",
    "\n",
    "<br>\n",
    "\n",
    "Before we build our model, there are a few steps we should take before we build and fit our model. \n",
    "\n",
    "First of all, with K-Means, Heirarchical, and Spectral Clustering, it behooves us to standardize the data first (normalize the data by dividing by subtracting the mean from each value and then dividing, according to variable columns). Since all of these methods depend on some distance or similarity measure, we want to make sure each variable is being treated equally, so the results won't be over-infuenced by certain variables simply for being on a bigger measurement scale. \n",
    "\n",
    "Secondly, we need to convert our data into a matrix. All **'.fit(x)'** methods require a numpy array as input, and won't accept a pandas dataframe. Luckily, the **'scale(x)'** function accomplishes both goals. \n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_Matrix = preprocessing.scale(DATA)\n",
    "#returns a scaled version of the dataframe, in numpy array/matrix form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now we can start model building! \n",
    "\n",
    "<br>\n",
    "\n",
    "##K-Means\n",
    "\n",
    "Recall that the K means clustering algorithm has three main model parameters: \n",
    "\n",
    "  * Distance Measure Between Points\n",
    "  * Cluster Center Measure\n",
    "  * Number of Clusters\n",
    "  \n",
    "And one main computational parameter: \n",
    "\n",
    "  * Number of Iterations\n",
    "  \n",
    "Sci-Kit Learn uses **Euclidean Distance** as the distance measure, and **mean** as the center measure. You can choose number of clusters (in fact, the model requires it, duh!) and number of iterations to compute (default is 300), however, the first two parameters are set, unfortunately, and cannot be changed. \n",
    "\n",
    "So let's start!\n",
    "\n",
    "<br>\n",
    "\n",
    "First, we create an empty model (so no data yet) along with the parameters we'll use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Model_K = KMeans(n_clusters=3)\n",
    "#creates an empty model object, with set parameters, but not fit to data yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we fit the model with the data using object method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Model_K.fit(DATA_Matrix)\n",
    "#fits the model using data from the data matrix. Now, that the model has been fit, it will have additional object\n",
    "#methods and attributes available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model has been fit, the cluster labels are stored in an object attribute called .labels_ as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Model_K.labels_\n",
    "#calling the newly available object attribute .labels_ returns a numpy array with the cluster assignments for each\n",
    "#observation, matched by index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! Easy as that. We can go ahead and attach the cluster labels back to the observations, for context, but all the actual work is finished! 3 steps! Isn't Sci-Kit Learn great?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultKmeans = rawDATA[['Cereal Name']][:]\n",
    "#we create new object, a single column dataframe with just the observations' names. Note how we select the column \n",
    "#'Cereal Name' from the original dataframe, and then put a [:] at the end of the call. Doing so ensure we're creating\n",
    "#a new object, otherwise resultKmeans would simply be pointing to the original dataframe, and we don't want to do that\n",
    "#since we'll be appending it below\n",
    "\n",
    "resultKmeans['Cluster Labels K Means'] = Model_K.labels_\n",
    "#joins the cluster labels from K Means to the dataframe of observation's names, created above\n",
    "\n",
    "resultKmeans[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast and easy! \n",
    "\n",
    "####Note on K-Means\n",
    "\n",
    "The real drawback to K-Means in Sci-Kit Learn is that it's not very versitile. We can't control the distance or center measures. However, the next two clustering functions allow more flexibility. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Agglomerative Hierarchical\n",
    "\n",
    "So with agglomerative hierarchical clustering, we have two main parameters for creating the dendogram:\n",
    "\n",
    "  * Distance Measure\n",
    "  * Linkage Function Between Clusters\n",
    "\n",
    "And an additional two parametes/choices for extracting clusters from the dendogram:\n",
    "\n",
    "  * Cluster Selection Method\n",
    "  * Number of Clusters\n",
    "  \n",
    "Fortunately, Sci-Kit learn gives us some leeway with choosing these parameters. Distance measure can be selected using **affinity** (default: 'euclidean') and linkage function can be selected using **linkage** (default: 'ward'). There are additional sub-parameters which you can set depending on the linkage function (for example, a ward linkage function also requires a pooling function). \n",
    "\n",
    "For cluster assignment, we need to define the number of clusters we want, of course, but Sci-Kit Learn doesn't allow you to select a cluster selection method. They simply use the top down approach, start at the top of the dendogram and move down until you have k clusters. \n",
    "\n",
    "Also important to note, the agglomerative hierarchical clustering algorithm in Sci-Kit Learn is built for cluster assignment. It builds a dendogram in order to assign clusters, but it doesn't allow you to see or access the dendogram, so if you wanted to, lets say, build a dendogram, and analyze it, instead of choosing some pre-determined k clusters, you would need to use a different function. I believe scipy has a function for this, but the proof is left to the reader. \n",
    "\n",
    "One more note, Sci-Kit learn also has a parameter, **connectivity**, which allows you to pre-structure the data according to some connectivity matrix. Not sure what this means, and again, the proof is left to the reader. \n",
    "\n",
    "<br>\n",
    "\n",
    "So Lets begin! Again, create the empty model with the model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Model_Agglo = AgglomerativeClustering(n_clusters = 3, affinity = 'euclidean', linkage = 'complete')\n",
    "#creates empty agglomerative clustering object. Parameters are selected, here, we change the linkage function from \n",
    "#the default to complete linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we fit the model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Model_Agglo.fit(DATA_Matrix)\n",
    "#fits our previously empty model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the cluster labels! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Model_Agglo.labels_\n",
    "#cluster labels assigned to observations, by index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets put it into our results dataframe and compare against K-Means!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultKmeans['Cluster Labels Agglo'] = Model_Agglo.labels_\n",
    "#joins the cluster labels from Agglo to our previously created dataframe\n",
    "\n",
    "resultKmeans[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our Agglomerative Clustering model produced different results then K Means did. \n",
    "\n",
    "<br> \n",
    "\n",
    "####Note on Agglomerative Clustering\n",
    "\n",
    "With Agglomerative clustering, the linkage function we choose can have enormous impact on the dendogram, and hence, the cluster assignments produced. For example, Ward Linkage will minimize variance within groups, while Complete Linkage will make it harder for clusters to merge with each one another the further away the extremes of the clusters are away from each other, respectively. \n",
    "\n",
    "Because of this, Agglomerative Clustering can be versitile for different situations, depending on the data and context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Spectral Clustering\n",
    "\n",
    "Finally, Spectral Clustering. Spectral Clustering is similar to Agglomerative in that there are multiple phases: \n",
    "\n",
    "  * 1: Create an affinity/similarity matrix of the observations\n",
    "  * 2: Generate the Laplacian\n",
    "  * 3: Cluster the results\n",
    "  \n",
    "Hence, there is a lot of room for variation. The parameters are as follows:\n",
    "\n",
    "  * Similarity Measure for computing affinity/similarity matrix\n",
    "    * Many possible methods: KNN, RBF, etc. \n",
    "    * Any parameters associated with those methods\n",
    "  * Clustering method\n",
    "    * Possible methods: K-Means, Discretized\n",
    "    * Any parameters of those methods\n",
    "    * Number of clusters\n",
    "    \n",
    "The Spectral Clustering function for Sci-Kit Learn allows for pretty high customization of these parameters. The affinity/similarity matrix calculation can be set with **affinity** (default: 'RBF')- NOTE: affinity here is defined a little differently then from the affinity in the Agglomerative Clustering function. In the Agglomerative Clustering function, affinity is the measure used to calculate distance between points, whereas here, it's the measure used to compute an affinity/similarity matrix. END NOTE - The clustering method can be assigned using **assign_labels** (default: 'kmeans'). There are also parameters specfici to certain affinity and assign_label choices which only activate if the associated method is chosen. \n",
    "\n",
    "Again, as before, the number of clusters needs to be selected. \n",
    "\n",
    "One thing to note is that although one could run spectral clustering on either the observations of a dataset or the variables of a dataset, Spectral Clustering will by default treat the observations as the data being assigned clusters, and will compute the affinity/similarity matrix accordingly. \n",
    "\n",
    "So let's begin! \n",
    "\n",
    "<br>\n",
    "\n",
    "First, begin, as usual, by creating the empty model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Model_SpookyGhost = SpectralClustering(n_clusters=3, eigen_solver= 'arpack', \n",
    "                                       affinity= 'nearest_neighbors', n_neighbors= 4, assign_labels= 'discretize')\n",
    "#creates empty spectral clustering model, with set parameters. \n",
    "#this model is using KNN with K=4 to create the affinity matrix, and then assigning clusters using a \n",
    "#discretized approach. The 'eigen_solver' parameter specifies which linear algebra function to use to compute the \n",
    "#eigen-stuff\n",
    "#Model is called SpookyGhost because spectres are ghosts and ghosts are spooky. \n",
    "#James Bond was also a choice, but I decided it was too topical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we fit the model. \n",
    "\n",
    "NOTE: we use the .fit(x) method as with the K-Means and Agglomerative, however, Spectral Clustering has another object method available: .fit_predict(x), which is slightly different. The .fit_predict(x) method does two things differently. \n",
    "  * 1: It doesn't compute a affinity/similarity matrix from the data, instead, it just clusters the data directly. \n",
    "  * 2: It directly outputs the cluster labels, instead of changing the object and storing the results in an object attribute. \n",
    "  \n",
    "Since we want to run the full spectral clustering algorithm to our data, we'll use .fit(x) instead of .fit_predict(x). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Model_SpookyGhost.fit(DATA_Matrix)\n",
    "#fits the model using our data. \n",
    "#note that we are using .fit(), NOT .fit_predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we can get our cluster labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Model_SpookyGhost.labels_\n",
    "#returns our cluster labels, according to index. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets throw it into our results dataframe and see how they all compare!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultKmeans['Cluster Labels Spectre'] = Model_SpookyGhost.labels_\n",
    "#joins the cluster labels from SpookyGhost to our previously created dataframe\n",
    "\n",
    "resultKmeans[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you go. Notice how all of these methods produce different results. Each clustering algorithm has it's own peculiarities, and parameter choices can definitely impact how the data is labeled. The appropriate method to use depends on the context and data being clustered, and there is still no consensus on the appropriate way to determine this. \n",
    "\n",
    "####Note on Spectral Clustering\n",
    "\n",
    "Spectral Clustering can be quite different from K-Means Clustering in that while K-Means clusters according to closeness to some central point, Spectral clusters according to the similarity of the data to each other. However, the degree to which this is true is a function of the parameter choices selected. The parameter choices in Spectral Clustering can have enormous impact on how the algorithm behaves. If, for example, RBF was used instead of KNN, or KNN used a large K, and K-Means was used instead of Discretized, the algorithm can often behave similarly to K-Means, grouping data together based common region instead of connectivity to near points. \n",
    "\n",
    "The parameters used in our example were chosen for their tendency to make the model assign datapoints chain-linked together the same label, so it handles spirals and such data better. However, this is just one aspect of Spectral Clustering's range of behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
